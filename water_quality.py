# -*- coding: utf-8 -*-
"""Water Quality using DL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mH9lcsiecNV7Y8yKiwber1TOaCBGQSIq
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from sklearn.model_selection import cross_val_score
import seaborn as sns
import matplotlib.pyplot as plt

# Load and preprocess data
def preprocess_data(file_path):
    data = pd.read_csv(file_path)
    data.fillna(data.mean(), inplace=True)
    data.drop_duplicates(inplace=True)
    return data

# Load dataset
file_path = '/content/sample_data/waterDataset.csv'
water_data = preprocess_data(file_path)

# Visualize data distributions
def visualize_data(data):
    for column in data.columns:
        plt.figure(figsize=(6, 4))
        sns.histplot(data[column], kde=True)
        plt.title(f'Distribution of {column}')
        plt.show()

visualize_data(water_data)

# Function to plot a heatmap
def plot_heatmap(data):
    # Calculate correlations
    correlation_matrix = data.corr()

    # Set up the matplotlib figure
    plt.figure(figsize=(12, 8))

    # Draw the heatmap
    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)

    # Adding title
    plt.title('Correlation Heatmap')

    # Show plot
    plt.show()

# Call the function to plot the heatmap
plot_heatmap(water_data)

# Prepare features and targets
def prepare_features_target(data):
    X = data.drop('Potability', axis=1)
    y = data['Potability']
    return X, y

X, y = prepare_features_target(water_data)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=404)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Define existing models
rf_clf = RandomForestClassifier(n_estimators=100, random_state=404)
gb_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=404)
mlp_clf = MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=404)

# Define a simple deep learning model
def create_dl_model(input_shape):
    model = Sequential([
        Dense(128, activation='relu', input_shape=input_shape),
        Dropout(0.2),
        Dense(64, activation='relu'),
        Dropout(0.2),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

dl_model = create_dl_model((X_train_scaled.shape[1],))

# Train and evaluate models
def train_evaluate_model(model, X_train, y_train, X_test, y_test, is_dl_model=False):
    if is_dl_model:
        model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)
        y_pred = (model.predict(X_test) > 0.5).astype("int32")
    else:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"{model.__class__.__name__ if not is_dl_model else 'DeepLearningModel'} Accuracy: {accuracy}")

# Train all models
for model in [rf_clf, gb_clf, mlp_clf]:
    train_evaluate_model(model, X_train_scaled, y_train, X_test_scaled, y_test)

train_evaluate_model(dl_model, X_train_scaled, y_train, X_test_scaled, y_test, is_dl_model=True)

# Updated deep learning model
def create_advanced_dl_model(input_shape):
    model = Sequential([
        Dense(256, activation='relu', input_shape=input_shape),
        Dropout(0.3),
        Dense(128, activation='relu'),
        Dropout(0.3),
        Dense(64, activation='relu'),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

# Create and train the updated model
advanced_dl_model = create_advanced_dl_model((X_train_scaled.shape[1],))
advanced_dl_model.fit(X_train_scaled, y_train, epochs=150, batch_size=64, verbose=1)

from tensorflow.keras.layers import BatchNormalization

def create_complex_model(input_shape):
    model = Sequential([
        Dense(512, activation='relu', input_shape=input_shape, kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        BatchNormalization(),
        Dropout(0.5),
        Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        BatchNormalization(),
        Dropout(0.5),
        Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
        BatchNormalization(),
        Dropout(0.5),
        Dense(64, activation='relu'),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

complex_model = create_complex_model((X_train_scaled.shape[1],))

from tensorflow.keras.optimizers import Adam

# Example of using a different optimizer and learning rate
optimizer = Adam(learning_rate=0.0001)
complex_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

# Training with early stopping and model checkpointing
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
model_checkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)

history = complex_model.fit(X_train_scaled, y_train, epochs=50, batch_size=64, verbose=1, validation_split=0.2, callbacks=[early_stopping, model_checkpoint])